@misc{Allen2019,
   abstract = {Bacterial growth presents many beautiful phenomena that pose new theoretical challenges to statistical physicists, and are also amenable to laboratory experimentation. This review provides some of the essential biological background, discusses recent applications of statistical physics in this field, and highlights the potential for future research.},
   author = {Rosalind J. Allen and Bartlomiej Waclaw},
   doi = {10.1088/1361-6633/aae546},
   issn = {00344885},
   issue = {1},
   journal = {Reports on Progress in Physics},
   keywords = {bacteria,microbiology,population dynamics,statistical physics},
   month = {1},
   pmid = {30270850},
   publisher = {Institute of Physics Publishing},
   title = {Bacterial growth: A statistical physicist's guide},
   volume = {82},
   year = {2019},
}
@article{Pinto2022,
   abstract = {We propose a compressed logistic model for bacterial growth by invoking a time-dependent rate instead of the intrinsic growth rate (constant), which was adopted in traditional logistic models. The new model may have a better physiological basis than the traditional ones, and it replicates experimental observations, such as the case example for E. coli, Salmonella, and Staphylococcus aureus. Stochastic colonial growth at a different rate may have a fractal-like nature, which should be an origin of the time-dependent reaction rate. The present model, from a stochastic viewpoint, is approximated as a Gaussian time evolution of bacteria (error function).},
   author = {Carlito Pinto and Koichi Shimakawa},
   doi = {10.1088/1478-3975/ac8c15},
   issn = {14783975},
   issue = {6},
   journal = {Physical Biology},
   keywords = {bacteria,complex systems,logistic growth,time-dependent growth rate},
   month = {11},
   pmid = {35998621},
   publisher = {Institute of Physics},
   title = {A compressed logistic equation on bacteria growth: inferring time-dependent growth rate},
   volume = {19},
   year = {2022},
}
@article{,
   abstract = {This work proposes a new mathematical model describing the dynamics of growing bacterial cultures. The model, described by a first order non-linear differential equation, as a generalization of the logistic equation, was compared with the most studied mathematical models. All models were numerically implemented and fitted to the experimental data, collected from the incubation of a bacterial strain of Pseudomonas fluorescens, to obtain the growth parameters. The experimental data showed the lowest fit error for both the Baranyi–Roberts and new models, which turned out to be equivalent. Simulations of the fitting algorithm were also implemented and repeated for a large number of initial guesses of the parameters, chosen in order to test the fitting and convergence performances. The innovative feature that makes the new model easier to use than Baranyi–Roberts model is definitely its simple and manageable analytical form and its good performance in terms of convergence time.},
   author = {Anna Lo Grasso and Ada Fort and Fariba Fahmideh Mahdizadeh and Agnese Magnani and Chiara Mocenni},
   doi = {10.1080/13873954.2023.2236681},
   issn = {17445051},
   issue = {1},
   journal = {Mathematical and Computer Modelling of Dynamical Systems},
   keywords = {differential equation,mathematical model,parameter estimation},
   pages = {169-185},
   publisher = {Taylor and Francis Ltd.},
   title = {Generalized logistic model of bacterial growth},
   volume = {29},
   year = {2023},
}
@article{Frenkel2021,
   abstract = {The outcome of an antibiotic treatment on the growth capacity of bacteria is largely dependent on the initial population size (Inoculum Effect). We characterized and built a model of this effect in E. coli cultures using a large variety of antimicrobials, including conventional antibiotics, and for the first time, cationic antimicrobial peptides (CAMPs). Our results show that all classes of antimicrobial drugs induce an inoculum effect, which, as we explain, implies that the dynamic is bistable: For a range of anti-microbial densities, a very small inoculum decays whereas a larger inoculum grows, and the threshold inoculum depends on the drug concentration. We characterized three distinct classes of drug-induced bistable growth dynamics and demonstrate that in rich medium, CAMPs correspond to the simplest class, bacteriostatic antibiotics to the second class, and all other traditional antibiotics to the third, more complex class. These findings provide a unifying universal framework for describing the dynamics of the inoculum effect induced by antimicrobials with inherently different killing mechanisms.},
   author = {Nelly Frenkel and Ron Saar Dover and Eve Titon and Yechiel Shai and Vered Rom-Kedar},
   doi = {10.3390/antibiotics10010087},
   issn = {20796382},
   issue = {1},
   journal = {Antibiotics},
   keywords = {Antibiotics,Antimicrobial peptides,Inoculum effect,Mathematical modeling of infection,Microbiology},
   month = {1},
   pages = {1-20},
   publisher = {MDPI AG},
   title = {Bistable bacterial growth dynamics in the presence of antimicrobial agents},
   volume = {10},
   year = {2021},
}
@article{Greulich2017,
   abstract = {Understanding how antibiotics inhibit bacteria can help to reduce antibiotic use and hence avoid antimicrobial resistance - yet few theoretical models exist for bacterial growth inhibition by a clinically relevant antibiotic treatment regimen. In particular, in the clinic, antibiotic treatment is time-dependent. Here, we use a theoretical model, previously applied to steady-state bacterial growth, to predict the dynamical response of a bacterial cell to a time-dependent dose of ribosome-targeting antibiotic. Our results depend strongly on whether the antibiotic shows reversible transport and/or low-affinity ribosome binding ('low-affinity antibiotic') or, in contrast, irreversible transport and/or high affinity ribosome binding ('high-affinity antibiotic'). For low-affinity antibiotics, our model predicts that growth inhibition depends on the duration of the antibiotic pulse, and can show a transient period of very fast growth following removal of the antibiotic. For high-affinity antibiotics, growth inhibition depends on peak dosage rather than dose duration, and the model predicts a pronounced post-antibiotic effect, due to hysteresis, in which growth can be suppressed for long times after the antibiotic dose has ended. These predictions are experimentally testable and may be of clinical significance.},
   author = {Philip Greulich and Jakub Doležal and Matthew Scott and Martin R. Evans and Rosalind J. Allen},
   doi = {10.1088/1478-3975/aa8001},
   issn = {14783975},
   issue = {6},
   journal = {Physical Biology},
   keywords = {antibiotics,bacterial growth,dynamical systems,pharmacodynamics,ribosomes},
   month = {11},
   pmid = {28714461},
   publisher = {Institute of Physics Publishing},
   title = {Predicting the dynamics of bacterial growth inhibition by ribosome-targeting antibiotics},
   volume = {14},
   year = {2017},
}
@article{,
   abstract = {Growth of microorganisms and interpretation of growth data are core skills required by microbiologists. While science moves forward, it is of paramount importance that essential skills are not lost. The bacterial growth curve and the information that can gleaned from it is of great value to all of microbiology, whether this be a simple growth experiment, comparison of mutant strains or the establishment of conditions for a large-scale multi-omics experiment. Increasingly, the basics of plotting and interpreting growth curves and growth data are being overlooked. This primer article serves as a refresher for microbiologists on the fundamentals of microbial growth kinetics.},
   author = {Lorena T. Fernández-Martínez and Arnaud Javelle and Paul A. Hoskisson},
   doi = {10.1099/mic.0.001428},
   issn = {14652080},
   issue = {2},
   journal = {Microbiology (United Kingdom)},
   keywords = {bacterial growth,chemostat,growth curve. 001428},
   pmid = {38329407},
   publisher = {Microbiology Society},
   title = {Microbial Primer: Bacterial growth kinetics},
   volume = {170},
   year = {2024},
}
@misc{Petrungaro2021,
   abstract = {Antibiotic resistance is a growing public health problem. To gain a fundamental understanding of resistance evolution, a combination of systematic experimental and theoretical approaches is required. Evolution experiments combined with next-generation sequencing techniques, laboratory automation, and mathematical modeling are enabling the investigation of resistance development at an unprecedented level of detail. Recent work has directly tracked the intricate stochastic dynamics of bacterial populations in which resistant mutants emerge and compete. In addition, new approaches have enabled measuring how prone a large number of genetically perturbed strains are to evolve resistance. Based on advances in quantitative cell physiology, predictive theoretical models of resistance are increasingly being developed. Taken together, a new strategy for observing, predicting, and ultimately controlling resistance evolution is emerging.},
   author = {Gabriela Petrungaro and Yuval Mulla and Tobias Bollenbach},
   doi = {10.1016/j.coisb.2021.100365},
   issn = {24523100},
   journal = {Current Opinion in Systems Biology},
   keywords = {Antibiotic resistance,Evolution experiments,High-throughput experiments,Mathematical modeling,Population dynamics},
   month = {12},
   publisher = {Elsevier Ltd},
   title = {Antibiotic resistance: Insights from evolution experiments and mathematical modeling},
   volume = {28},
   year = {2021},
}
@article{Wang2024,
   abstract = {While the popularity of physics-informed neural networks (PINNs) is steadily rising, to this date PINNs have not been successful in simulating dynamical systems whose solution exhibits multi-scale, chaotic or turbulent behavior. In this work we attribute this shortcoming to the inability of existing PINNs formulations to respect the spatio-temporal causal structure that is inherent to the evolution of physical systems. We argue that this is a fundamental limitation and a key source of error that can ultimately steer PINN models to converge towards erroneous solutions. We address this pathology by proposing a simple re-formulation of PINNs loss functions that can explicitly account for physical causality during model training. We demonstrate that this simple modification alone is enough to introduce significant accuracy improvements, as well as a practical quantitative mechanism for assessing the convergence of a PINNs model. We provide state-of-the-art numerical results across a series of benchmarks for which existing PINNs formulations fail, including the chaotic Lorenz system, the Kuramoto–Sivashinsky equation in the chaotic regime, and the Navier–Stokes equations. To the best of our knowledge, this is the first time that PINNs have been successful in simulating such systems, introducing new opportunities for their applicability to problems of industrial complexity.},
   author = {Sifan Wang and Shyam Sankaran and Paris Perdikaris},
   doi = {10.1016/j.cma.2024.116813},
   issn = {00457825},
   journal = {Computer Methods in Applied Mechanics and Engineering},
   keywords = {Chaotic systems,Computational physics,Deep learning,Partial differential equations},
   month = {3},
   publisher = {Elsevier B.V.},
   title = {Respecting causality for training physics-informed neural networks},
   volume = {421},
   year = {2024},
}
@article{Wang2022,
   abstract = {Although Physics-Informed Neural Networks (PINNs) have been successfully applied in a wide variety of science and engineering fields, they can fail to accurately predict the underlying solution in slightly challenging convection-diffusion-reaction problems. In this paper, we investigate the reason of this failure from a domain distribution perspective, and identify that learning multi-scale fields simultaneously makes the network unable to advance its training and easily get stuck in poor local minima. We show that the widespread experience of sampling more collocation points in high-loss layer regions hardly help optimize and may even worsen the results. These findings motivate the development of a novel curriculum learning method that encourages neural networks to prioritize learning on easier non-layer regions while downplaying learning on harder layer regions. The proposed method helps PINNs automatically adjust the learning emphasis and thereby facilitate the optimization procedure. Numerical results on typical benchmark equations show that the proposed curriculum learning approach mitigates the failure modes of PINNs and can produce accurate results for very sharp boundary and interior layers. Our work reveals that for equations whose solutions have large scale differences, paying less attention to high-loss regions can be an effective strategy for learning them accurately.},
   author = {Yufeng Wang and Cong Xu and Min Yang and Jin Zhang},
   month = {10},
   title = {Less Emphasis on Difficult Layer Regions: Curriculum Learning for Singularly Perturbed Convection-Diffusion-Reaction Problems},
   url = {http://arxiv.org/abs/2210.12685},
   year = {2022},
}
@article{Monaco2023,
   abstract = {Physics-informed neural networks (PINNs) are gaining popularity as powerful tools for solving nonlinear Partial Differential Equations (PDEs) through Deep Learning. PINNs are trained by incorporating physical laws as soft constraints in the loss function. Such an approach is effective for trivial equations, but fails in solving various classes of more complex dynamical systems. In this work, we put on the test bench three state-of-the-art PINN training methods for solving three popular Partial Differential Equations (PDEs) of increasing complexity, besides the additional application of the Fourier Feature Embedding (FFE), and the introduction of a novel implementation of Curriculum regularization. Experiments evaluate the convergence of the trained PINN and its prediction error rate for different training sizes and training lengths (i.e., number of epochs). To provide an overview of the behaviour of each learning method, we introduce a new metric, named overall score. Our experiments show that a given approach can either be the best in all situations or not converge at all. The same PDE can be solved with different learning methods, which in turn give the best results, depending on the training size or the use of FFE. From our experiments we conclude that there is no learning method to train them all, yet we extract useful patterns that can drive future works in this growing area of research. All code and data of this manuscript are publicly available on GitHub.},
   author = {Simone Monaco and Daniele Apiletti},
   doi = {10.1016/j.rineng.2023.101023},
   issn = {25901230},
   journal = {Results in Engineering},
   keywords = {Computational physics,Deep learning,Partial differential equations},
   month = {6},
   publisher = {Elsevier B.V.},
   title = {Training physics-informed neural networks: One learning to rule them all?},
   volume = {18},
   year = {2023},
}
@article{Dekhovich2024,
   abstract = {Physics-informed neural networks (PINNs) have recently become a powerful tool for solving partial differential equations (PDEs). However, finding a set of neural network parameters that fulfill a PDE at the boundary and within the domain of interest can be challenging and non-unique due to the complexity of the loss landscape that needs to be traversed. Although a variety of multi-task learning and transfer learning approaches have been proposed to overcome these issues, no incremental training procedure has been proposed for PINNs. As demonstrated herein, by developing incremental PINNs (iPINNs) we can effectively mitigate such training challenges and learn multiple tasks (equations) sequentially without additional parameters for new tasks. Interestingly, we show that this also improves performance for every equation in the sequence. Our approach learns multiple PDEs starting from the simplest one by creating its own subnetwork for each PDE and allowing each subnetwork to overlap with previously learned subnetworks. We demonstrate that previous subnetworks are a good initialization for a new equation if PDEs share similarities. We also show that iPINNs achieve lower prediction error than regular PINNs for two different scenarios: (1) learning a family of equations (e.g., 1-D convection PDE); and (2) learning PDEs resulting from a combination of processes (e.g., 1-D reaction–diffusion PDE). The ability to learn all problems with a single network together with learning more complex PDEs with better generalization than regular PINNs will open new avenues in this field.},
   author = {Aleksandr Dekhovich and Marcel H.F. Sluiter and David M.J. Tax and Miguel A. Bessa},
   doi = {10.1007/s00366-024-02010-1},
   issn = {14355663},
   journal = {Engineering with Computers},
   keywords = {Incremental learning,Physic-informed neural networks (PINNs),Scientific machine learning (SciML),Sparsity},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {iPINNs: incremental learning for Physics-informed neural networks},
   year = {2024},
}
@article{,
   title = {GitHub - kotserge_pinn-curriculum-learning_ Physics Informed Neural Networks (PINNs) for solving Convection-Diffusion Equation using Curriculum Regularization},
}
@misc{,
   abstract = {Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.},
   author = {Aditi S Krishnapriyan and Amir Gholami and Shandian Zhe and Robert M Kirby and Michael W Mahoney},
   title = {Characterizing possible failure modes in physics-informed neural networks},
}
@article{Wang2023,
   abstract = {Physics-informed neural networks (PINNs) have been popularized as a deep learning framework that can seamlessly synthesize observational data and partial differential equation (PDE) constraints. Their practical effectiveness however can be hampered by training pathologies, but also oftentimes by poor choices made by users who lack deep learning expertise. In this paper we present a series of best practices that can significantly improve the training efficiency and overall accuracy of PINNs. We also put forth a series of challenging benchmark problems that highlight some of the most prominent difficulties in training PINNs, and present comprehensive and fully reproducible ablation studies that demonstrate how different architecture choices and training strategies affect the test accuracy of the resulting models. We show that the methods and guiding principles put forth in this study lead to state-of-the-art results and provide strong baselines that future studies should use for comparison purposes. To this end, we also release a highly optimized library in JAX that can be used to reproduce all results reported in this paper, enable future research studies, as well as facilitate easy adaptation to new use-case scenarios.},
   author = {Sifan Wang and Shyam Sankaran and Hanwen Wang and Paris Perdikaris},
   month = {8},
   title = {An Expert's Guide to Training Physics-informed Neural Networks},
   url = {http://arxiv.org/abs/2308.08468},
   year = {2023},
}
@article{Brunton2016,
   abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
   author = {Steven L. Brunton and Joshua L. Proctor and J. Nathan Kutz},
   doi = {10.1073/pnas.1517384113},
   issn = {10916490},
   issue = {15},
   journal = {Proceedings of the National Academy of Sciences of the United States of America},
   keywords = {Dynamical systems,Machine learning,Optimization,Sparse regression,System identification},
   pages = {3932-3937},
   publisher = {National Academy of Sciences},
   title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
   volume = {113},
   year = {2016},
}
@misc{Blanco2024,
   author = {Omar Anoud Blanco and Gabriel Torregrosa and Cortés Jordi and García Ojalvo},
   month = {7},
   title = {Autoencoder analysis of bacterial growth},
   year = {2024},
}
@article{,
   abstract = {The aim of this study was to evaluate the suitability of several mathematical functions for describing microbial growth curves. The nonlinear functions used were: three-phase linear, logistic, Gompertz, Von Bertalanffy, Richards, Morgan, Weibull, France and Baranyi. Two data sets were used, one comprising 21 growth curves of different bacterial and fungal species in which growth was expressed as optical density units, and one comprising 34 curves of colony forming units counted on plates of Yersinia enterocolitica grown under different conditions of pH, temperature and CO2 (time-constant conditions for each culture). For both sets, curves were selected to provide a wide variety of shapes with different growth rates and lag times. Statistical criteria used to evaluate model performance were analysis of residuals (residual distribution, bias factor and serial correlation) and goodness-of-fit (residual mean square, accuracy factor, extra residual variance F-test, and Akaike's information criterion). The models showing the best overall performance were the Baranyi, three-phase linear, Richards and Weibull models. The goodness-of-fit attained with other models can be considered acceptable, but not as good as that reached with the best four models. Overall, the Baranyi model showed the best behaviour for the growth curves studied according to a variety of criteria. The Richards model was the best-fitting optical density data, whereas the three-phase linear showed some limitations when fitting these curves, despite its consistent performance when fitting plate counts. Our results indicate that the common use of the Gompertz model to describe microbial growth should be reconsidered critically, as the Baranyi, three-phase linear, Richards and Weibull models showed a significantly superior ability to fit experimental data than the extensively used Gompertz. © 2004 Elsevier B.V. All rights reserved.},
   author = {S. López and M. Prieto and J. Dijkstra and M. S. Dhanoa and J. France},
   doi = {10.1016/j.ijfoodmicro.2004.03.026},
   issn = {01681605},
   issue = {3},
   journal = {International Journal of Food Microbiology},
   keywords = {Growth curves,Mathematical models,Microbial growth,Nonlinear equations,Sigmoidal functions},
   month = {11},
   pages = {289-300},
   pmid = {15454319},
   title = {Statistical evaluation of mathematical models for microbial growth},
   volume = {96},
   year = {2004},
}
@misc{Cranmer2023,
   abstract = {PySR 3 is an open-source library for practical symbolic regression, a type of machine learning which aims to discover human-interpretable symbolic models. PySR was developed to democra-tize and popularize symbolic regression for the sciences, and is built on a high-performance distributed backend, a flexible search algorithm, and interfaces with several deep learning packages. PySR's internal search algorithm is a multi-population evolutionary algorithm, which consists of a unique evolve-simplify-optimize loop, designed for optimization of unknown scalar constants in newly-discovered empirical expressions. PySR's backend is an extremely optimized Julia library SymbolicRegression.jl 4 , which can be used directly from Julia. It is capable of fusing user-defined operators into SIMD kernels at runtime, performing automatic differentiation, and distributing populations of expressions to thousands of cores across a cluster. In describing this software, we also introduce a new benchmark, "EmpiricalBench," to quantify the applicability of symbolic regression algorithms in science. This benchmark measures recovery of historical empirical equations from original and synthetic datasets.},
   author = {Miles Cranmer},
   title = {Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl},
   year = {2023},
}
@article{Raissi2019,
   abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
   author = {M. Raissi and P. Perdikaris and G. E. Karniadakis},
   doi = {10.1016/j.jcp.2018.10.045},
   issn = {10902716},
   journal = {Journal of Computational Physics},
   keywords = {Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,Runge–Kutta methods},
   month = {2},
   pages = {686-707},
   publisher = {Academic Press Inc.},
   title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
   volume = {378},
   year = {2019},
}
@misc{Mehta2019,
   abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias–variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton–proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.},
   author = {Pankaj Mehta and Marin Bukov and Ching Hao Wang and Alexandre G.R. Day and Clint Richardson and Charles K. Fisher and David J. Schwab},
   doi = {10.1016/j.physrep.2019.03.001},
   issn = {03701573},
   journal = {Physics Reports},
   month = {5},
   pages = {1-124},
   publisher = {Elsevier B.V.},
   title = {A high-bias, low-variance introduction to Machine Learning for physicists},
   volume = {810},
   year = {2019},
}
@article{Prokop2024,
   abstract = {Many dynamical systems exhibit oscillatory behavior that can be modeled with differential equations. Recently, these equations have increasingly been derived through data-driven methods, including the transparent technique known as Sparse Identification of Nonlinear Dynamics (SINDy). This paper illustrates the importance of accurately determining the system's limit cycle position in phase space for identifying sparse and effective models. We introduce a method for identifying the limit cycle position and the system's nullclines by applying SINDy to datasets adjusted with various offsets. This approach is evaluated using three criteria: model complexity, coefficient of determination, and generalization error. We applied this method to several models: the oscillatory FitzHugh-Nagumo model, a more complex model consisting of two coupled cubic differential equations with a single stable state, and a multistable model of glycolytic oscillations. Our results confirm that incorporating detailed information about the limit cycle in phase space enhances the accuracy of model identification in oscillatory systems.e space can improve the success of model identification efforts in oscillatory systems.},
   author = {Bartosz Prokop and Nikita Frolov and Lendert Gelens},
   doi = {10.1063/5.0199311},
   month = {2},
   title = {Enhancing model identification with SINDy via nullcline reconstruction},
   url = {http://arxiv.org/abs/2402.03168 http://dx.doi.org/10.1063/5.0199311},
   year = {2024},
}
@article{Zhang2024,
   abstract = {Misfolded tau proteins play a critical role in the progression and pathology of Alzheimer's disease. Recent studies suggest that the spatio-temporal pattern of misfolded tau follows a reaction–diffusion type equation. However, the precise mathematical model and parameters that characterize the progression of misfolded protein across the brain remain incompletely understood. Here, we use deep learning and artificial intelligence to discover a mathematical model for the progression of Alzheimer's disease using longitudinal tau positron emission tomography from the Alzheimer's Disease Neuroimaging Initiative database. Specifically, we integrate physics informed neural networks (PINNs) and symbolic regression to discover a reaction–diffusion type partial differential equation for tau protein misfolding and spreading. First, we demonstrate the potential of our model and parameter discovery on synthetic data. Then, we apply our method to discover the best model and parameters to explain tau imaging data from 46 individuals who are likely to develop Alzheimer's disease and 30 healthy controls. Our symbolic regression discovers different misfolding models f(c) for two groups, with a faster misfolding for the Alzheimer's group, f(c)=0.23c3−1.34c2+1.11c, than for the healthy control group, f(c)=−c3+0.62c2+0.39c. Our results suggest that PINNs, supplemented by symbolic regression, can discover a reaction–diffusion type model to explain misfolded tau protein concentrations in Alzheimer's disease. We expect our study to be the starting point for a more holistic analysis to provide image-based technologies for early diagnosis, and ideally early treatment of neurodegeneration in Alzheimer's disease and possibly other misfolding-protein based neurodegenerative disorders.},
   author = {Zhen Zhang and Zongren Zou and Ellen Kuhl and George Em Karniadakis},
   doi = {10.1016/j.cma.2023.116647},
   issn = {00457825},
   journal = {Computer Methods in Applied Mechanics and Engineering},
   keywords = {Alzheimer's disease,Misfolded tau protein,Model discovery,PINNs,Symbolic regression,Uncertainty quantification},
   month = {2},
   publisher = {Elsevier B.V.},
   title = {Discovering a reaction–diffusion model for Alzheimer's disease by combining PINNs with symbolic regression},
   volume = {419},
   year = {2024},
}
